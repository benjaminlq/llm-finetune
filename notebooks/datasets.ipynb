{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/QUAN/Desktop/llm-finetune/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import LlamaTokenizer\n",
    "from utils import create_llama2_chat_prompt, save_dataset_to_json, count_tokens, create_llama2_instruction_prompt\n",
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import openai\n",
    "import re\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import asyncio\n",
    "\n",
    "MAIN_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "    \n",
    "openai.api_key = api_keys[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\", cache_dir=os.path.join(MAIN_DIR, \"model\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_string_by_index(string:str, repl: str, start_idx, end_idx):\n",
    "    prefix = string[:start_idx]\n",
    "    suffix = string[end_idx:]\n",
    "    return prefix + repl + suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Guanaco Chat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Download from HF Hub\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "datasets = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dataset = DatasetDict()\n",
    "\n",
    "for split in datasets:\n",
    "    chat_queries = []\n",
    "\n",
    "    for query in datasets[split][\"text\"]:\n",
    "        message_list = re.split(\"### Human: |### Assistant: \", query)[1:]\n",
    "        chat_query = create_llama2_chat_prompt(\n",
    "            message_list, hf_tokenizer\n",
    "        )\n",
    "        chat_queries.append(chat_query)\n",
    "        \n",
    "    chat_dataset[split] = Dataset.from_dict({\"text\": chat_queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 15.10ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 26.30ba/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset_to_json(chat_dataset, os.path.join(DATA_DIR, \"guanaco\", \"chat\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Tokens and Generate Groundtruths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdr_df = pd.read_csv(os.path.join(DATA_DIR, \"sdr\", \"Clindoc_masked_with_classes.csv\"))\n",
    "masked_texts = sdr_df[\"masked_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a dataset generator of a diabetes dataset in Singapore.\n",
    "You always keep people identifiers confidential and only generate fictional entities.\n",
    "=====\n",
    "TASK:\n",
    "You are given a patient summary text. However, some of the texts have been masked. The masked texts are annotated with inside <category>.\n",
    "Your task is to replace the <category> with a FICTIONAL entity based on the context of the summary to generate a synthetic dataset.\n",
    "The descriptions for each type of masked token is as follow:\n",
    "\n",
    "<Person Name>: Name of a person/human. Can be full name, short form, with or without salutation (Dr, Mr, Ms) \n",
    "<NRIC/Passport>: National Registration Identity Card (NRIC), Foreign Identity Numbers (FIN) and Passport Number\n",
    "<Medical Clinical Records>: Medical record numbers\n",
    "<Phone Number>: Telephone numbers\n",
    "<Email Address>: Electronic mail addresses\n",
    "<Home Address>: Patient home address (unit, level, block, street, full 6-digit postal codes).\n",
    "\n",
    "NOTE:\n",
    "- Only generate imaginary/fictional names (NOT ACTUAL doctors, phone numbers, email addresses, etc)\n",
    "- Filled information (names, IDs, phone, emails) must be in Singapore contexts \n",
    "- Try to generate a combination of both full names and short names.\n",
    "- Maintain the spaces and new lines of the original text.\n",
    "- If no token is present, return an empty list.\n",
    "- If there are duplicates, include all of them in the list\n",
    "=====\n",
    "Return a list which contains the token and the entity generated.\n",
    "=====\n",
    "EXAMPLES:\n",
    "MASKED PATIENT SUMMARY: <Person Name> <NRIC/Passport> 65/Malay /Male ADL Independent Community-Ambulant.\n",
    "ANSWER: [{{\"<Person Name>\": \"Jonathan Lee\"}}, {{\"<NRIC/Passport>\": \"S1234567D\"}}]\n",
    "\n",
    "MASKED PATIENT SUMMARY: <Person Name> 52 year old Malay Female NKDA ADL-independent, community ambulant with wheelchair on follow-up SGH Renal <Person Name>\n",
    "ANSWER: [{{\"<Person Name>\": \"Steven Lee\"}}, {{\"<Person Name>\": \"Steven Lee\"}}]\n",
    "\n",
    "MASKED PATIENT SUMMARY: 51/Chi/M Premorbidly ADLi, comm amb without aid PAST MEDICAL HISTORY RENAL HISTORY ESRF secondary to chronic GN s/p DDRT\n",
    "ANSWER: []\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "=====\n",
    "MASKED PATIENT SUMMARY: {query}\n",
    "\"\"\"\n",
    "\n",
    "FILL_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    messages = [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        HumanMessagePromptTemplate.from_template(user_prompt)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a dataset generator of a diabetes dataset in Singapore.\n",
    "You always keep people identifiers confidential and only generate fictional entities.\n",
    "=====\n",
    "TASK:\n",
    "You are given a a list of token type and the corresponding number of required generated tokens.\n",
    "Your task is to generate FICTIONAL entities of the corresponding <category> tokens to fill/replace the token inside a diabetic patient summary.\n",
    "The descriptions for each type of masked token is as follow:\n",
    "\n",
    "<Person Name>: Name of a person/human. Can be full name, short form, initials, with or without salutation (Dr, Mr, Ms) \n",
    "<NRIC/Passport>: National Registration Identity Card (NRIC), Foreign Identity Numbers (FIN) and Passport Number\n",
    "<Medical Clinical Records>: Medical record numbers\n",
    "<Phone Number>: Telephone numbers\n",
    "<Email Address>: Electronic mail addresses\n",
    "<Home Address>: Patient home address (unit, level, block, street, full 6-digit postal codes).\n",
    "\n",
    "NOTE:\n",
    "- Only generate imaginary/fictional names (NOT ACTUAL doctors, phone numbers, email addresses, etc)\n",
    "- Filled information (names, IDs, phone, emails) must be in Singapore contexts \n",
    "- Try to generate a combination of both full names and short names.\n",
    "- Maintain the spaces and new lines of the original text.\n",
    "- If no token is present, return an empty list.\n",
    "- If there are duplicates, include all of them in the list\n",
    "=====\n",
    "Return a list which contains the token and the entity generated.\n",
    "=====\n",
    "EXAMPLES:\n",
    "NUMBER OF TOKENS: \n",
    "- <Person Name>: 1\n",
    "- <NRIC/Passport>: 1\n",
    "- <Medical Clinical Records>: 0\n",
    "- <Phone Number>: 0\n",
    "- <Email Address>: 0\n",
    "- <Home Address>: 0\n",
    "ANSWER: [{{\"<Person Name>\": \"Jonathan Lee\"}}, {{\"<NRIC/Passport>\": \"S1234567D\"}}]\n",
    "\n",
    "EXAMPLES:\n",
    "NUMBER OF TOKENS: \n",
    "- <Person Name>: 1\n",
    "- <NRIC/Passport>: 1\n",
    "- <Medical Clinical Records>: 0\n",
    "- <Phone Number>: 0\n",
    "- <Email Address>: 0\n",
    "- <Home Address>: 0\n",
    "ANSWER: [{{\"<Person Name>\": \"Lee YM\"}}, {{\"<Person Name>\": \"Dr Tan MJ\"}}, {{\"<Person Name>\": \"Dr Lee Si Kiat\"}}]\n",
    "\n",
    "NUMBER OF TOKENS: \n",
    "- <Person Name>: 0\n",
    "- <NRIC/Passport>: 0\n",
    "- <Medical Clinical Records>: 0\n",
    "- <Phone Number>: 0\n",
    "- <Email Address>: 0\n",
    "- <Home Address>: 0\n",
    "ANSWER: []\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "=====\n",
    "NUMBER OF TOKENS: \n",
    "- <Person Name>: {name_no}\n",
    "- <NRIC/Passport>: {nric_no}\n",
    "- <Medical Clinical Records>: {mcr_no}\n",
    "- <Phone Number>: {phone_no}\n",
    "- <Email Address>: {email_no}\n",
    "- <Home Address>: {address_no}\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "FALLBACK_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    messages = [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        HumanMessagePromptTemplate.from_template(user_prompt)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_result_class(text):\n",
    "    llm = ChatOpenAI(model = \"gpt-4-1106-preview\", temperature = 1.0, max_tokens = 512)\n",
    "    class_fill_chain = LLMChain(llm=llm, prompt=FILL_PROMPT)\n",
    "    with get_openai_callback() as cb:\n",
    "        resp = await class_fill_chain.acall(text)\n",
    "    return resp[\"text\"], cb.total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.180749999999999\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for text in masked_texts:\n",
    "    result = generate_result_class(text)\n",
    "    results.append(result)\n",
    "    \n",
    "results = await asyncio.gather(*results)\n",
    "ground_truths = [result[0] for result in results]\n",
    "total_cost = sum([result[1] for result in results])\n",
    "\n",
    "print(total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "TOKEN2ID = {\n",
    "    \"<Person Name>\": 0,\n",
    "    \"<NRIC/Passport>\": 1,\n",
    "    \"<Medical Clinical Records>\": 2,\n",
    "    \"<Phone Number>\": 3,\n",
    "    \"<Email Address>\": 4,\n",
    "    \"<Home Address>\": 5\n",
    "}\n",
    "\n",
    "replacement_tokens = []\n",
    "\n",
    "for text_idx, text in enumerate(ground_truths):\n",
    "    token_counter = np.zeros((len(TOKEN2ID)), dtype=int)\n",
    "    for idx, token in enumerate(TOKEN2ID.keys()):\n",
    "        token_counter[idx] = len(re.findall(token, masked_texts[text_idx]))\n",
    "\n",
    "    gpt_counter = np.zeros((len(TOKEN2ID)), dtype=int)\n",
    "\n",
    "    ground_truth = re.findall(r\"\\[[^\\[\\]]*\\]\", text)[0]\n",
    "    token_list = eval(text)\n",
    "\n",
    "    for gpt_token in token_list:\n",
    "        for k in gpt_token:\n",
    "            gpt_counter[TOKEN2ID[k]] += 1\n",
    "                \n",
    "    if not np.array_equal(gpt_counter, token_counter):\n",
    "        print(text_idx)\n",
    "        llm = ChatOpenAI(model = \"gpt-4-1106-preview\", temperature = 1.0, max_tokens = 512)\n",
    "        class_fill_chain = LLMChain(llm=llm, prompt=FALLBACK_PROMPT)\n",
    "        refined_response = class_fill_chain(\n",
    "            {\n",
    "                \"name_no\": token_counter[0],\n",
    "                \"nric_no\": token_counter[1],\n",
    "                \"mcr_no\": token_counter[2],\n",
    "                \"phone_no\": token_counter[3],\n",
    "                \"email_no\": token_counter[4],\n",
    "                \"address_no\": token_counter[5]\n",
    "                }\n",
    "            )\n",
    "        ground_truth = re.findall(r\"\\[[^\\[\\]]*\\]\", refined_response[\"text\"])[0]\n",
    "        token_list = eval(text)\n",
    "        gpt_counter = np.zeros((len(TOKEN2ID)), dtype=int)\n",
    "        for gpt_token in token_list:\n",
    "            for k in gpt_token:\n",
    "                gpt_counter[TOKEN2ID[k]] += 1\n",
    "        \n",
    "        assert np.array_equal(gpt_counter, token_counter), \"Not equal\"\n",
    "    \n",
    "    assert isinstance(eval(text), list)\n",
    "    replacement_tokens.append(token_list)\n",
    "    ground_truths[text_idx] = ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_texts = []\n",
    "span_lists = []\n",
    "for text_idx, (entities, masked_text) in enumerate(zip(entity_list, masked_texts)):\n",
    "    gen_text = masked_text\n",
    "    span_list = []\n",
    "    for entity_dict in entities:\n",
    "        for entity_type, entity in entity_dict.items():\n",
    "            match_obj = re.search(entity_type, gen_text)\n",
    "            token_span = match_obj.span()\n",
    "            start_index = token_span[0] - 15 if token_span[0] - 15 >= 0 else 0\n",
    "            end_index = token_span[1] + 15\n",
    "            print(text_idx, gen_text[start_index:end_index])\n",
    "            gen_text = replace_string_by_index(gen_text, entity, token_span[0], token_span[1])\n",
    "            entity_span = (token_span[0], token_span[0] + len(entity))\n",
    "            span_list.append(entity_span)\n",
    "    span_lists.append(span_list)\n",
    "    gen_texts.append(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, \"sdr\", \"gt.json\"), \"w\") as f:\n",
    "    json.dump(ground_truths, f)\n",
    "    \n",
    "with open(os.path.join(DATA_DIR, \"sdr\", \"gen_annotations.json\"), \"w\") as f:\n",
    "    json.dump(span_lists, f)\n",
    "    \n",
    "gen_df = pd.DataFrame({\"text\": gen_texts})\n",
    "\n",
    "gen_df.to_csv(os.path.join(DATA_DIR, \"sdr\", \"Clindoc_gen.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate finetune and inference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a administrator working with diabete patients data.\n",
    "===\n",
    "TASK:\n",
    "Given a patient summary text, identify relevant Protected Health Information (PHI). The PHI can belong to the following categories:\n",
    "\n",
    "<Person Name>: Name of a person/human. Can be full name, short form, initials, with or without salutation (Dr, Mr, Ms) \n",
    "<NRIC/Passport>: National Registration Identity Card (NRIC), Foreign Identity Numbers (FIN) and Passport Number\n",
    "<Medical Clinical Records>: Medical record numbers\n",
    "<Phone Number>: Telephone numbers\n",
    "<Email Address>: Electronic mail addresses\n",
    "<Home Address>: Patient home address (unit, level, block, street, full 6-digit postal codes).\n",
    "\n",
    "If no entities is present, return an empty list.\n",
    "If there are duplicates, include all of them in the list\n",
    "===\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "PATIENT SUMMARY: {query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  7.39ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 165.86ba/s]\n"
     ]
    }
   ],
   "source": [
    "chat_prompts = []\n",
    "inst_prompts = []\n",
    "\n",
    "for ground_truth, query in zip(ground_truths, gen_texts):\n",
    "    inst_prompt = create_llama2_instruction_prompt(\n",
    "        system_prompt.format(),\n",
    "        user_prompt.format(query=query),\n",
    "        ground_truth,\n",
    "        prompt_template=\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    )\n",
    "    chat_prompt = create_llama2_chat_prompt(\n",
    "        system_prompt=system_prompt.format(),\n",
    "        messages = [user_prompt.format(query=query), ground_truth],\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    chat_prompts.append(chat_prompt)\n",
    "    inst_prompts.append(inst_prompt)\n",
    "    \n",
    "inst_dataset = Dataset.from_dict({\"text\": inst_prompts})\n",
    "chat_dataset = Dataset.from_dict({\"text\": chat_prompts})\n",
    "finetune_dataset = DatasetDict({\"chat\": chat_dataset,\"inst\": inst_dataset})\n",
    "\n",
    "save_dataset_to_json(finetune_dataset, os.path.join(DATA_DIR, \"sdr\", \"finetune\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2040\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 0\n",
    "for chat_prompt in chat_prompts:\n",
    "    token_count = count_tokens(chat_prompt, tokenizer)\n",
    "    if token_count > max_tokens:\n",
    "        max_tokens = token_count\n",
    "\n",
    "print(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 137.12ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 138.65ba/s]\n"
     ]
    }
   ],
   "source": [
    "eval_chat_prompts = []\n",
    "eval_inst_prompts = []\n",
    "\n",
    "for query in gen_texts:\n",
    "    inst_prompt = create_llama2_instruction_prompt(\n",
    "        system_prompt.format(),\n",
    "        user_prompt.format(query=query),\n",
    "        prompt_template=\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    )\n",
    "    chat_prompt = create_llama2_chat_prompt(\n",
    "        system_prompt=system_prompt.format(),\n",
    "        messages = [user_prompt.format(query=query)],\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    eval_chat_prompts.append(chat_prompt)\n",
    "    eval_inst_prompts.append(inst_prompt)\n",
    "\n",
    "inst_dataset = Dataset.from_dict({\"text\": eval_inst_prompts})\n",
    "chat_dataset = Dataset.from_dict({\"text\": eval_chat_prompts})\n",
    "eval_dataset = DatasetDict({\"chat\": chat_dataset, \"inst\": inst_dataset})\n",
    "\n",
    "save_dataset_to_json(eval_dataset, os.path.join(DATA_DIR, \"sdr\", \"eval\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'information_extraction', 'description': 'Extracts the relevant information from the passage.', 'parameters': {'type': 'object', 'properties': {'info': {'type': 'array', 'items': {'type': 'object', 'properties': {'person_name': {'title': 'person_name', 'type': 'string'}, 'person_height': {'title': 'person_height', 'type': 'integer'}, 'person_hair_color': {'title': 'person_hair_color', 'type': 'string'}, 'dog_name': {'title': 'dog_name', 'type': 'string'}, 'dog_breed': {'title': 'dog_breed', 'type': 'string'}}, 'required': []}}}, 'required': ['info']}}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_extraction_chain\n",
    "from langchain.chains.openai_functions.extraction import _get_extraction_function\n",
    "\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"person_name\": {\"type\": \"string\"},\n",
    "        \"person_height\": {\"type\": \"integer\"},\n",
    "        \"person_hair_color\": {\"type\": \"string\"},\n",
    "        \"dog_name\": {\"type\": \"string\"},\n",
    "        \"dog_breed\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [],\n",
    "}\n",
    "\n",
    "# ner_chain = create_extraction_chain(\n",
    "#     llm=ChatOpenAI(model=\"gpt-4-1106-preview\", temperature = 0, max_tokens = 512),\n",
    "#     prompt=None,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "print(_get_extraction_function(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a doctor who treats diabetes patients in Singapore.\n",
    "One of your job scope is to write patient summary which contains information related to patient treatments.\n",
    "=====\n",
    "TASK:\n",
    "You are given a patient summary text. You need to identify relevant Protected Health Information (PHI) for anonymisation pipeline.\n",
    "Your task is to perform Name Entity Recognition on the given text and extract from the given text ALL relevant entities which belong to the following categories:\n",
    "\n",
    "1. People name (Full name and Short Form - e.g. Mr Kim, Dr Lee)\n",
    "2. Telephone and Fax numbers\n",
    "3. Electronic mail addresses\n",
    "4. National Registration Identity Card (NRIC), Foreign Identity Numbers (FIN) and Passport Number\n",
    "5. Medical record numbers\n",
    "6. Account numbers\n",
    "7. Certificate/license numbers\n",
    "8. Vehicle identifiers & license plate numbers\n",
    "9. Device identifiers\n",
    "10. Web Universal Resource Locators (URLs) and Internet Protocol (IP) address numbers\n",
    "11. Patient home address (unit, level, block, street, full postal codes). Exclude if the initial four digits of a postal code or hospital addresses.\n",
    "12. Date of birth (DOB) ONLY. Do not extract dates of clinical meetings, operations, etc.\n",
    "\n",
    "=====\n",
    "OUTPUT INSTRUCTIONS:\n",
    "Your output should be a list of JSON objects which contains \"entity\" and \"entity_type\". Entity type should be an integer corresponding the class number listed above.\n",
    "=====\n",
    "EXAMPLES:\n",
    "PATIENT SUMMARY: Madam Tan 73 year old Chinese female DA: Erythromycin Stays with husband, son and helper Baseline occasionally able to shake and nod in response to questions according to son Past Medical History 1. End stage renal failure secondary to diabetic nephropathy - on HD 1,3,5 via left BC AVF 2. Diabetes mellitus - last HbA1c 6.2% (December 2018) 3. Hypertension 4. Hyperlipidaemia 5. Steal syndrome - status post L BC AVF creation (20/2/12) by Prof Lim Boon Leng and team\n",
    "OUTPUT JSON:\n",
    "[{{\"entity\": \"Madam Tan\", \"entity_type\": 1}},{{\"entity\": \"Prof Lim Boon Leng\", \"entity_type\": 1}}]\n",
    "\n",
    "PATIENT SUMMARY: 75 year old Chinese female ADLs assisted, WC bound lives with domestic helper and family NKDA === PMHX === 1. DM 2. HTN 3. Hyperlipidemia 4. IHD/TVD - s/p PCI (Mar 2014) DES prox-mid LAD - Last 2DE (Mar 2014): EF 57% mild-mod MR - MIBI (May 2014): mild ischaemia in basal infero-lateral wall of LV 5. ESRF secondary to DM on HD 2/4/6 via R BC AVF 6. R eye blindness 7. Previous ICH 8. Previous hx of biliary colic\n",
    "OUTPUT JSON:\n",
    "[] \n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"=====\n",
    "PATIENT SUMMARY: {query}\n",
    "\"\"\"\n",
    "\n",
    "#Note: Keep the entities in the exact form as the original texts (no rephrasing, correction, or change in capital letters, etc)\n",
    "\n",
    "\n",
    "EXTRACT_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    messages = [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        HumanMessagePromptTemplate.from_template(user_prompt)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_chain = LLMChain(\n",
    "    llm=ChatOpenAI(model = \"gpt-4-1106-preview\", temperature = 0, max_tokens = 512),\n",
    "    prompt=EXTRACT_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.966900000000006\n"
     ]
    }
   ],
   "source": [
    "ner_results = []\n",
    "\n",
    "async def generate_ner_result(text):\n",
    "    with get_openai_callback() as cb:\n",
    "        resp = await extract_chain.acall(text)\n",
    "    return resp[\"text\"], cb.total_cost\n",
    "\n",
    "for text in unmasked_text:\n",
    "    result = generate_ner_result(text)\n",
    "    ner_results.append(result)\n",
    "    \n",
    "ner_results = await asyncio.gather(*ner_results)\n",
    "gpt_preds = [result[0] for result in ner_results]\n",
    "total_cost = sum([result[1] for result in ner_results])\n",
    "print(\"Total Cost: {:.3f}\".format(total_cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdr_df[\"preds_3\"] = gpt_preds\n",
    "\n",
    "sdr_df.to_csv(\n",
    "    os.path.join(DATA_DIR, \"sdr\", \"Clindoc_recovered.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = refined_texts[0]\n",
    "sample_gt_response = gpt_preds[0]\n",
    "\n",
    "is_chat = True\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\", cache_dir=os.path.join(MAIN_DIR, \"model\"),\n",
    "    )\n",
    "\n",
    "if is_chat:\n",
    "    prompt_str = create_llama2_chat_prompt(\n",
    "        messages=[\n",
    "            user_prompt.format(query = sample_query),\n",
    "            sample_gt_response\n",
    "            ],\n",
    "        tokenizer=tokenizer, system_prompt=system_prompt\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are a doctor who treats diabetes patients in Singapore.\n",
      "One of your job scope is to write patient summary which contains information related to patient treatments.\n",
      "=====\n",
      "TASK:\n",
      "You are given a patient summary text. You need to identify relevant Protected Health Information (PHI) for anonymisation pipeline.\n",
      "Your task is to perform Name Entity Recognition on the given text and extract from the given text ALL relevant entities which belong to the following categories:\n",
      "\n",
      "1. People name (Full name and Short Form - e.g. Mr Kim, Dr Lee)\n",
      "2. Telephone and Fax numbers\n",
      "3. Electronic mail addresses\n",
      "4. National Registration Identity Card (NRIC), Foreign Identity Numbers (FIN) and Passport Number\n",
      "5. Medical record numbers\n",
      "6. Account numbers\n",
      "7. Certificate/license numbers\n",
      "8. Vehicle identifiers & license plate numbers\n",
      "9. Device identifiers\n",
      "10. Web Universal Resource Locators (URLs) and Internet Protocol (IP) address numbers\n",
      "11. Patient home address (unit, level, block, street, full postal codes). Exclude if the initial four digits of a postal code or hospital addresses.\n",
      "12. Date of birth (DOB) ONLY. Do not extract dates of clinical meetings, operations, etc.\n",
      "\n",
      "=====\n",
      "OUTPUT INSTRUCTIONS:\n",
      "Your output should be a list of JSON objects which contains \"entity\" and \"entity_type\". Entity type should be an integer corresponding the class number listed above.\n",
      "=====\n",
      "EXAMPLES:\n",
      "PATIENT SUMMARY: Madam Tan 73 year old Chinese female DA: Erythromycin Stays with husband, son and helper Baseline occasionally able to shake and nod in response to questions according to son Past Medical History 1. End stage renal failure secondary to diabetic nephropathy - on HD 1,3,5 via left BC AVF 2. Diabetes mellitus - last HbA1c 6.2% (December 2018) 3. Hypertension 4. Hyperlipidaemia 5. Steal syndrome - status post L BC AVF creation (20/2/12) by Prof Lim Boon Leng and team\n",
      "OUTPUT JSON:\n",
      "[{{\"entity\": \"Madam Tan\", \"entity_type\": 1}},{{\"entity\": \"Prof Lim Boon Leng\", \"entity_type\": 1}}]\n",
      "\n",
      "PATIENT SUMMARY: 75 year old Chinese female ADLs assisted, WC bound lives with domestic helper and family NKDA === PMHX === 1. DM 2. HTN 3. Hyperlipidemia 4. IHD/TVD - s/p PCI (Mar 2014) DES prox-mid LAD - Last 2DE (Mar 2014): EF 57% mild-mod MR - MIBI (May 2014): mild ischaemia in basal infero-lateral wall of LV 5. ESRF secondary to DM on HD 2/4/6 via R BC AVF 6. R eye blindness 7. Previous ICH 8. Previous hx of biliary colic\n",
      "OUTPUT JSON:\n",
      "[] \n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "=====\n",
      "PATIENT SUMMARY: Dr Tan's patient    At last visit: noted labile BP from 60 - 160 systolic  On atenolol 25mg OM, nifedipine LA 30mg OM   In clinic; BP 63/45 --> 84/55 --> 10/60 --> 110/67  Atenolol stopped  ANS study done: pseudomotor and cardiovascular dysautonomia  Also noted hyperkalemia     Today:   Hba1c 6.6%  FBG 6.8  K 5.7, Cr 107  LDL 2.71  Renin/Aldo pending, ? hyporeninemic hypoaldosteronism contributing to hyper K     Had a fall in Oct, hit forehead. Felt giddy after standing up   Home BP 80-120systolic. Not high recently     P:  Start fludrocortisone low-dose, for both ANS dysfunction and hyperK  Continue nifedipine for now  Watch for LL swelling, supine hypertension   Trace renin/Aldo levels   TCU Dr Tan  KIV for ARB if K ok               Imp:  Autonomic dysfunction resulting in labile BP   Hyperkalemia - ? hyporeninemic hypoaldosteronism   - hypocortisolism excluded already    P:  Stop atenolol  Ok if BP goes up transiently to 160 systolic plus  Autonomic nervous system study  Resonium x 2 days  Reduce metformin and statin (patient's request)    7/12/17;  Aldo / renin traced; WNL. Not hyporeninemic hypoaldosteronsim    Called patient's wife  She found BP slightly elevated (up to 170systolic) with fludrocortisone so stopped it  Self-adjusted nifedipine LA - now taking 30mg at noon and finds BP more within normal range  But still occ when BP drops to 80 systolic    P:  Continue nifedipine LA 30mg at noon  Monitor - KIV restart fludrocortisone 0.025mg if persistent low BP\n",
      " [/INST] [{\"entity\": \"Dr Tan\", \"entity_type\": 1}] </s>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
