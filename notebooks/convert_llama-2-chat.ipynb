{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/QUAN/Desktop/llm-finetune/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama.llama.tokenizer import Tokenizer as OriginalTokenizer\n",
    "from transformers import LlamaTokenizer as HFTokenizer\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from typing import Literal, List, TypedDict, Optional\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "MAIN_DIR = \"..\"\n",
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Llama implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "class Message(TypedDict):\n",
    "    role: Role\n",
    "    content: str\n",
    "\n",
    "class CompletionPrediction(TypedDict, total=False):\n",
    "    generation: str\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n",
    "\n",
    "\n",
    "class ChatPrediction(TypedDict, total=False):\n",
    "    generation: Message\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n",
    "\n",
    "Dialog = List[Message]\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "SPECIAL_TAGS = [B_INST, E_INST, \"<<SYS>>\", \"<</SYS>>\"]\n",
    "UNSAFE_ERROR = \"Error: special tags are not allowed as part of the prompt.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = OriginalTokenizer(\n",
    "    os.path.join(MAIN_DIR, \"model\", \"original-llama-tokenizer\", \"tokenizer.model\")\n",
    "    )\n",
    "\n",
    "sentencepiece_vocab = {\n",
    "    id : llama_tokenizer.sp_model.id_to_piece(id)\n",
    "    for id in range(llama_tokenizer.sp_model.get_piece_size())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Last message must be from user, got assistant",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m([msg[\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m msg \u001b[39min\u001b[39;00m dialog[::\u001b[39m2\u001b[39m]]) \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     [msg[\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m msg \u001b[39min\u001b[39;00m dialog[\u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m]]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m ), (\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel only supports \u001b[39m\u001b[39m'\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m'\u001b[39m\u001b[39m roles, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstarting with \u001b[39m\u001b[39m'\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, then \u001b[39m\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and alternating (u/a/u/a/u...)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m dialog_tokens: List[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     [\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m         llama_tokenizer\u001b[39m.\u001b[39mencode(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     [],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     dialog[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLast message must be from user, got \u001b[39m\u001b[39m{\u001b[39;00mdialog[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m dialog_tokens \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m llama_tokenizer\u001b[39m.\u001b[39mencode(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mB_INST\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m(dialog[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mstrip()\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mE_INST\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     bos\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     eos\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/QUAN/Desktop/llm-finetune/notebooks/convert_llama-2-chat.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Last message must be from user, got assistant"
     ]
    }
   ],
   "source": [
    "max_gen_len = 256\n",
    "\n",
    "unsafe_requests = []\n",
    "\n",
    "dialog = [\n",
    "    Message(role= \"system\", content=\"This is a system message\"),\n",
    "    Message(role= \"user\", content=\"What's 1 + 1?\"),\n",
    "    Message(role= \"assistant\", content=\"2\"),\n",
    "    Message(role = \"user\", content=\"Are you sure?\")\n",
    "    ]\n",
    "\n",
    "unsafe_requests.append(\n",
    "    any([tag in msg[\"content\"] for tag in SPECIAL_TAGS for msg in dialog])\n",
    ")\n",
    "if dialog[0][\"role\"] == \"system\":\n",
    "    dialog = [\n",
    "        {\n",
    "            \"role\": dialog[1][\"role\"],\n",
    "            \"content\": B_SYS\n",
    "            + dialog[0][\"content\"]\n",
    "            + E_SYS\n",
    "            + dialog[1][\"content\"],\n",
    "        }\n",
    "    ] + dialog[2:]\n",
    "assert all([msg[\"role\"] == \"user\" for msg in dialog[::2]]) and all(\n",
    "    [msg[\"role\"] == \"assistant\" for msg in dialog[1::2]]\n",
    "), (\n",
    "    \"model only supports 'system', 'user' and 'assistant' roles, \"\n",
    "    \"starting with 'system', then 'user' and alternating (u/a/u/a/u...)\"\n",
    ")\n",
    "dialog_tokens: List[int] = sum(\n",
    "    [\n",
    "        llama_tokenizer.encode(\n",
    "            f\"{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} \",\n",
    "            bos=True,\n",
    "            eos=True,\n",
    "        )\n",
    "        for prompt, answer in zip(\n",
    "            dialog[::2],\n",
    "            dialog[1::2],\n",
    "        )\n",
    "    ],\n",
    "    [],\n",
    ")\n",
    "\n",
    "assert (\n",
    "    dialog[-1][\"role\"] == \"user\"\n",
    "), f\"Last message must be from user, got {dialog[-1]['role']}\"\n",
    "\n",
    "dialog_tokens += llama_tokenizer.encode(\n",
    "    f\"{B_INST} {(dialog[-1]['content']).strip()} {E_INST}\",\n",
    "    bos=True,\n",
    "    eos=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of tokens:  51\n",
      "<s>▁[INST]▁<<SYS>><0x0A>This▁is▁a▁system▁message<0x0A><</SYS>><0x0A><0x0A>What's▁1▁+▁1?▁[/INST]▁2▁</s><s>▁[INST]▁Are▁you▁sure?▁[/INST]\n"
     ]
    }
   ],
   "source": [
    "print(\"No of tokens: \", len(dialog_tokens))\n",
    "print(\"\".join([sentencepiece_vocab[id] for id in dialog_tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Tokenizer\n",
    "\n",
    "- **With System**:\\<s> [INST] <\\<SYS>>\\nSystem message\\n<\\</SYS>>\\n\\n**User1** [/INST] **Assistant1** \\</s>\\<s> [INST] **User2** [/INST]``\n",
    "\n",
    "- **Without System**: \\<s> [INST] **User1** [/INST] **Assistant1** \\</s>\\<s> [INST] **User2** [/INST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = HFTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\", token=api_keys[\"HF_API_KEY\"], cache_dir=os.path.join(MAIN_DIR, \"model\"),\n",
    "    add_bos_token = False, add_eos_token = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llama2_chat_prompt(\n",
    "    messages: List[str], tokenizer: PreTrainedTokenizer, system_prompt: Optional[str] = None\n",
    ") -> str:\n",
    "    prompt_str = \"\"\n",
    "\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    BOS_TOKEN, EOS_TOKEN = tokenizer.bos_token, tokenizer.eos_token\n",
    "\n",
    "    if system_prompt:\n",
    "        messages[0] = B_SYS + system_prompt + E_SYS + messages[0]\n",
    "        \n",
    "    for user_message, assistant_message in zip(messages[::2], messages[1::2]):\n",
    "        user_message = BOS_TOKEN + \" \" + B_INST + \" \" + user_message + \" \" + E_INST\n",
    "        assistant_message = \" \" + assistant_message + \" \" + EOS_TOKEN\n",
    "        \n",
    "        prompt_str += user_message\n",
    "        prompt_str += assistant_message\n",
    "\n",
    "    if (len(messages) % 2) == 1:       \n",
    "        user_query = BOS_TOKEN + \" \" + B_INST + \" \" + messages[-1] + \" \" + E_INST\n",
    "        prompt_str += user_query\n",
    "    \n",
    "    return prompt_str.strip()\n",
    "\n",
    "ALPACA_INSTRUCTION_COMPLETION_TEMPLATE = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "ALPACA_INSTRUCTION_TEMPLATE_NO_INPUT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def create_llama2_instruction_prompt(\n",
    "    instruction: str,\n",
    "    input: Optional[str] = None,\n",
    "    output: Optional[str] = None,\n",
    "    prompt_template: str = ALPACA_INSTRUCTION_COMPLETION_TEMPLATE\n",
    ") -> str:\n",
    "    if not input:\n",
    "        prompt_template = ALPACA_INSTRUCTION_TEMPLATE_NO_INPUT\n",
    "        prompt_str = prompt_template.format(instruction = instruction)\n",
    "        \n",
    "    else:\n",
    "        prompt_str = prompt_template.format(instruction=instruction, input=input)\n",
    "    \n",
    "    if output:\n",
    "        prompt_str += f\"\\n{output}\"\n",
    "            \n",
    "    return prompt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"This is a system message\"\n",
    "messages = [\n",
    "    \"What's 1 + 1?\",\n",
    "    \"2\",\n",
    "    \"Are you sure?\",\n",
    "]\n",
    "\n",
    "prompt_str = create_llama2_chat_prompt(messages, hf_tokenizer, system_prompt)\n",
    "hf_tokens = hf_tokenizer.encode(prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "np.array_equal(np.array(hf_tokens), np.array(dialog_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
